{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import cv2\n",
    "import tqdm\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('DataSplit.pickle', 'rb') as handle:\n",
    "    DataSplit = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Senz3DDataset(Dataset):\n",
    "    def __init__(self, paths, labels):\n",
    "        self.paths = paths\n",
    "        self.labels = labels\n",
    "        self.GT_labels = {'1': 5, '2': 2, '3': 3, '4': 5, '5': 0, '6': 2, \n",
    "                          '7': 1, '8': 4, '9': 3, '10': 1, '11': 1}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(\"..\\\\.\" + self.paths[idx])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (img.shape[1]//4, img.shape[0]//4)).astype(np.uint8)\n",
    "        \n",
    "        label = torch.tensor(self.GT_labels[(self.labels[idx])])\n",
    "        \n",
    "        return {'img': img, 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestDataset = Senz3DDataset(list(DataSplit['test'].keys()), list(DataSplit['test'].values()))\n",
    "TestDataLoader = DataLoader(TestDataset, batch_size=22, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_test(test_loader, model, loss_fn, device):\n",
    "    test_total = 0\n",
    "    test_correct = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, sample in enumerate(test_loader):\n",
    "\n",
    "            # move inputs to device\n",
    "            if torch.cuda.is_available():\n",
    "                x = sample['img'].permute(0, 3, 1, 2).float().to(device)\n",
    "                y = sample['label'].to(device)\n",
    "\n",
    "            output = model.forward(x)\n",
    "\n",
    "            # Calculate accuracy/loss\n",
    "            _, y_hat = torch.max(output, dim=1)\n",
    "            batch_correct = torch.sum(y_hat == y)\n",
    "            test_correct += batch_correct\n",
    "            test_total += x.shape[0]\n",
    "\n",
    "    print(f'Test Acc: {test_correct.float()/test_total}')\n",
    "    return test_correct.float()/test_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 0.8211008906364441\n",
      "epoch_test_accuracy is:  tensor(0.8211, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "model_name = 'resnext50'\n",
    "model = models.resnext50_32x4d()\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(in_features=2048, out_features=1024, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64,6),\n",
    "    nn.Softmax(dim=0),    \n",
    "    )\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(\"saved_models/\" + model_name + \".pth\"))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "epoch_test_accuracy = eval_test(TestDataLoader, model, nn.CrossEntropyLoss(), device)\n",
    "print(\"Test Accuracy is: \", epoch_test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 0.8348623514175415\n",
      "epoch_test_accuracy is:  tensor(0.8349, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "model_name = 'resnet152'\n",
    "model = models.resnet152()\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(in_features=2048, out_features=1024, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64,6),\n",
    "    nn.Softmax(dim=0),    \n",
    "    )\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(\"saved_models/\" + model_name + \".pth\"))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "epoch_test_accuracy = eval_test(TestDataLoader, model, nn.CrossEntropyLoss(), device)\n",
    "print(\"epoch_test_accuracy is: \", epoch_test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 0.8853210806846619\n",
      "epoch_test_accuracy is:  tensor(0.8853, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "model_name = 'resnet18'\n",
    "model = models.resnet18()\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(in_features=512, out_features=1000, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1000, 500),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(500,50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50,6),\n",
    "    nn.Softmax(dim=0),    \n",
    "    )\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(\"saved_models/\" + model_name + \".pth\"))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "epoch_test_accuracy = eval_test(TestDataLoader, model, nn.CrossEntropyLoss(), device)\n",
    "print(\"epoch_test_accuracy is: \", epoch_test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why is the test accuracy so much lower than the validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the test dataset is balanced.... BUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.hist(list(DataSplit['test'].values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In truth it's not. If we look at the actual labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(iter(TestDataLoader))\n",
    "sample['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mapping of the test dataset should have been done based on the ground truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
